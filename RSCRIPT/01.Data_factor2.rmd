---
title: "01.Data Factoring"
output: github_document
---
Library Inclusion
```{r setup, include=FALSE}
Sys.setlocale("LC_CTYPE", "ko_KR.UTF-8")
library(keras)
library(KoNLP)
library(tm)
library(rJava)
library(dplyr)
```

## Data Files List

```{r}
cat('Loading data...\n')

setwd('/Users/Andrew-MB/OneDrive/êµìœ¡/ëŒ€í•™ì›/3/ë¹…ë°ì´í„°ì‘ìš©ì„¸ë¯¸ë‚˜/insta_crawl_data/cur_data/lstm_data')

file.info(dir('/Users/Andrew-MB/OneDrive/êµìœ¡/ëŒ€í•™ì›/3/ë¹…ë°ì´í„°ì‘ìš©ì„¸ë¯¸ë‚˜/insta_crawl_data/cur_data/lstm_data',
              pattern = 'csv'))
```

## Parameters

```{r}
max_features <- 20000
maxlen <- 100
batch_size <- 32

```

### 0. ì „ì²´ ë°ì´í„° ì¼ê´„ ìˆ˜ì§‘

```{r}
set.seed(1);
setwd('/Users/Andrew-MB/OneDrive/êµìœ¡/ëŒ€í•™ì›/3/ë¹…ë°ì´í„°ì‘ìš©ì„¸ë¯¸ë‚˜/insta_crawl_data/cur_data/lstm_data')

files <- list.files(pattern=".csv$")

raw_data <-  read.csv(files[1], encoding="UTF-8")

for (f in files[-1]){
  df <- read.csv(f, encoding="UTF-8")
  raw_data <- rbind(raw_data, df)
}

drops <- c("img","reg_date","write_date")
keeps <- c("id","text","has_tag")

raw_data <- raw_data[ , !(names(raw_data) %in% drops)]

# removing duplicated data
raw_data <- raw_data %>% 
         group_by(id, has_tag) %>%
         summarise(text = toString(unique(text)))


df <- do.call("rbind", lapply(raw_data$text, as.data.frame))

removePplTag <- function(x) {
    gsub("@[[:graph:]]*", " ", x)
}

removeURL <- function(x) {
    gsub("http://[[:graph:]]*", "", x)
    gsub("https://[[:graph:]]*", "", x)
}

removeSpace <- function(x) {
    gsub("[\r\n]", "", x)
    
}

removeDot <- function(x) {
  gsub("[.]", "", x)
}
df$text <- sapply(df$`X[[i]]`, removePplTag)
df$text <- sapply(df$text, removeURL)
df$text <- sapply(df$text, removeDot)
df$text <- sapply(df$text, removeSpace)
#df$text <- sapply(df$text, function(x) {
#  gsub("*#", " #", x)
#})
df$text <- sapply(df$text, function(x) {
  gsub("[[[:graph:]]]", "", x)
})
df$text <- sapply(df$text, function(x) {
  gsub("[#]", ' ', x)
})
#useSejongDic()

# export to CSV file


length(raw_data$has_tag[raw_data$has_tag =="ê¸°ì¨"])
length(raw_data$has_tag[raw_data$has_tag =="ìŠ¬í””"])
length(raw_data$has_tag[raw_data$has_tag =="ìš°ìš¸"])
length(raw_data$has_tag[raw_data$has_tag =="ì¦ê±°ì›€"])
length(raw_data$has_tag[raw_data$has_tag =="í™”ë‚¨"])


```

```{r}
test_df <- do.call("rbind", lapply(raw_data$has_tag, as.data.frame))

test_df$text <- sapply(df$text, function(x) {
  #return(utf8ToInt(x))
  return(x)
})

test_df$class <- sapply(test_df$`X[[i]]`, function(x) {
  #"ê¸°ì¨","ìŠ¬í””","ì¦ê±°ì›€","í™”ë‚¨","ìš°ìš¸"
  if(x == "ìš°ìš¸" | x == "í™”ë‚¨" | x == "ìŠ¬í””"){
    return(0)
  }
  
  return(1)
})



drops <- c("X[[i]]")
test_df <- test_df[ , !(names(test_df) %in% drops)]

write.csv(test_df, "/Users/Andrew-MB/DEV/05.GIT/GSCIT-sns-sentiment/MODEL/data/lstm_data171101.csv", row.names=FALSE)
```

```{r}

insta_data <- list()


#df$ktext <- sapply(df$text, customConvHangulToJamos)
#df$k2text <- sapply(df$text, customConvHangulToKeyStrokes)



#train.idx <- sample(nrow(test_df), ceiling(nrow(test_df) * 0.8))
train.idx <- sample(nrow(test_df), ceiling(nrow(test_df) * 0.5))
test.idx <- (1:nrow(test_df)) [- train.idx]


#test_df[train.idx,]$text

#utf8ToInt('\u2614')
insta_data <- list(
  train = list(
    x = rapply(test_df[train.idx,]$text, function(r) {
      return(r)
    }),
    y = lapply(test_df[train.idx,]$class, function(r) {
      return(r)
    })
  ),
  test = list(
    x = lapply(test_df[test.idx,]$text, function(r) {
      return(r)
    }),
    y = lapply(test_df[test.idx,]$class, function(r) {
      return(r)
    })
  )
)

rbind(c(2,3),c(4,5))
#insta_data$train$x
rbind(test_df[train.idx,]$text)
```

## parameter settings


```{r, echo = FALSE}
# Define training and test sets
x_train_insta <- insta_data$train$x
y_train_insta <- insta_data$train$y
x_test_insta <- insta_data$test$x
y_test_insta <- insta_data$test$y 

# Output lengths of testing and training sets
cat(length(x_train_insta), 'train sequences\n')
cat(length(x_test_insta), 'test sequences\n')
```

## Pad training and test inputs

```{r, echo = FALSE}
cat('Pad sequences (samples x time)\n')

# Pad training and test inputs
x_train_insta <- pad_sequences(x_train_insta, maxlen = 1000000)
x_test_insta <- pad_sequences(x_test_insta, maxlen = 1000000)

x_train[0]
x_train_insta[0]
```

### 1-1 ì „ì²˜ë¦¬, SNS í•œê¸€ ê²Œì‹œê¸€ ì¶”ì¶œ

```{r, echo = FALSE}
Hangul_Jamo <- as.u_char_range("1100..11FF")
Hangul_Com_Jamo <- as.u_char_range("3130..318F")
Hangul_Jamo_Ex_A <- as.u_char_range("A960..A97F")
Hangul_Syll <- as.u_char_range("AC00..D7AF")
Hangul_Jamo_Ex_B <- as.u_char_range("D7B0..D7FF")

only_korean_texts <- c()

for(te in insta_data[,2]){
  test_sent <- sapply(te, as.character)
  test_sent_split <- strsplit(test_sent, "")[[1]]
  
  for (ch in test_sent_split) {

    u_ch <- as.u_char(utf8ToInt(ch))
    if(!is.na(u_char_match(u_ch,Hangul_Com_Jamo)) ||
       !is.na(u_char_match(u_ch,Hangul_Jamo)) ||
       !is.na(u_char_match(u_ch,Hangul_Jamo_Ex_A)) ||
       !is.na(u_char_match(u_ch,Hangul_Jamo_Ex_B)) ||
       !is.na(u_char_match(u_ch,Hangul_Syll))
       ){
      
      only_korean_texts <-c(only_korean_texts, te)
      break
    }
  
  }
}

#df$text <- sapply(df$text, function(x) {
#    paste(extractNoun(x), collapse = " ")
#})

# build corpus
myCorpus_ <- Corpus(VectorSource(df$text))
myCorpus_ <- tm_map(myCorpus_, removePunctuation)
myCorpus_ <- tm_map(myCorpus_, stripWhitespace)
# myCorpus_ <- tm_map(myCorpus_, removeNumbers)
myCorpus_ <- tm_map(myCorpus_, tolower)
myStopwords <- c(stopwords("english"), "rt")
myCorpus_ <- tm_map(myCorpus_, removeWords, myStopwords)
```

### 1-1 ì „ì²˜ë¦¬, SNS í•œê¸€ ê²Œì‹œê¸€ ì¶”ì¶œ
```{r}


sns_type <- c("in", "fb")

# clean text 
cleanText <- function(corpus) {
  corpus.tmMap <- tm_map(corpus, removePunctuation)
  corpus.tmMap <- tm_map(corpus.tmMap, stripWhitespace)
  corpus.tmMap <- tm_map(corpus.tmMap, tolower)
  corpus.tmMap <- tm_map(corpus.tmMap, removeNumbers)
  corpus.tmMap <- tm_map(corpus.tmMap, removeWords, stopwords("english"))
  return(corpus.tmMap)
}

# generate TermDocumentMatrix
generateTDM <- function(sns_t) {
  print(sns_t)
  if(sns_t == "in"){
    s.cor <- Corpus(VectorSource(insta_data[,2]))
    # s.cor.cl <- cleanText(s.cor)
    s.tdm <- TermDocumentMatrix(s.cor)
    result <- list(name = sns_t, tdm = s.tdm)
  }else{
    result <- list(name = sns_t, tdm = "1")
  }
  
  # 
  # 
  # s.tdm <- removeSparseTerms(s.tdm, 0.8)
  #
  

}

insta_tdm <- lapply(sns_type, generateTDM)
```
### KoNLP í•¨ìˆ˜ë“¤ ê³µë¶€
```{r}

customConvHangulToJamos <- function(hangul){
  if(!is.character(hangul) | nchar(hangul) == 0){
    stop("Input must be legitimate character!")
  }else{
    jamos <- .jcall("kr/pe/freesearch/korean/KoHangul", "S","convertHangulStringToJamos",hangul,TRUE)
	  Encoding(jamos) <- "UTF-8" 
    return(unlist(strsplit(jamos,intToUtf8(0xFF5C))))
  }
}

customConvHangulToKeyStrokes <- function(hangul, isFullwidth=TRUE){
  if(!is.character(hangul) | nchar(hangul) == 0){
    stop("Input must be legitimate character!")
  }else{
    keystrokes <- .jcall("kr/pe/freesearch/korean/KoHangul", 
                         "S","convertHangulStringToKeyStrokes",hangul,isFullwidth,TRUE)
    Encoding(keystrokes) <- "UTF-8"
    #return(unlist(strsplit(keystrokes,intToUtf8(0xFF5C))))
    res<- gsub(" ", "", keystrokes)
    res<- gsub("ï½œ", "", res)
    return(res)
  } 
}

paste(customConvHangulToJamos("ë‹¹ë¶„ê°„ ì§„ì§œë¡œ ê¸ˆì£¼í•´ì•¼ì§€ğŸºğŸ»ğŸ¾ğŸ¥‚ğŸ‘ğŸ™… #ê¸ˆì£¼ #ëŒ€í•™ìƒ "), collapse = "")
paste(customConvHangulToKeyStrokes("ë‹¹ë¶„ê°„ ì§„ì§œë¡œ ê¸ˆì£¼í•´ì•¼ì§€ğŸºğŸ»ğŸ¾ğŸ¥‚ğŸ‘ğŸ™…#ê¸ˆì£¼ #ëŒ€í•™ìƒ "),
      collapse = "")
paste(customConvHangulToKeyStrokes("ë‹¹ë¶„ê°„ ì§„ì§œë¡œ ê¸ˆì£¼í•´ì•¼ì§€ğŸºğŸ»ğŸ¾ğŸ¥‚ğŸ‘ğŸ™…#ê¸ˆì£¼ #ëŒ€í•™ìƒ "), collapse = "")
```

### CSVíŒŒì¼ë¡œ ë°ì´í„° ì¶”ì¶œ

```{r}
f <- function(x, output){
  wellID <- 1
  print(paste(wellID, x[1], x[2], sep=","))
  cat(paste(wellID, x[1], x[2], sep=","), file= output, append = T, fill = T)
}
apply(DF, 1, f, output = 'outputfile.csv')
```

